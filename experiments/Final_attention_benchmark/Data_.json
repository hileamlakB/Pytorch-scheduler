{
    "nodes": [
        {
            "flops": 1073741824.0,
            "ops": [
                "aten._scaled_dot_product_efficient_attention"
            ],
            "inputs": [
                {
                    "dtype": "torch.float16",
                    "size": [
                        "32",
                        "8",
                        "128",
                        "64"
                    ],
                    "stride": [
                        "65536",
                        "8192",
                        "64",
                        "1"
                    ]
                },
                {
                    "dtype": "torch.float16",
                    "size": [
                        "32",
                        "8",
                        "128",
                        "64"
                    ],
                    "stride": [
                        "65536",
                        "8192",
                        "64",
                        "1"
                    ]
                },
                {
                    "dtype": "torch.float16",
                    "size": [
                        "32",
                        "8",
                        "128",
                        "64"
                    ],
                    "stride": [
                        "65536",
                        "8192",
                        "64",
                        "1"
                    ]
                }
            ],
            "kwargs": [
                {}
            ],
            "triton": {
                "kernel_name": "aten._scaled_dot_product_efficient_attention",
                "kernel_path": ""
            }
        },
        {
            "flops": 1073741824.0,
            "ops": [
                "aten._scaled_dot_product_efficient_attention"
            ],
            "inputs": [],
            "kwargs": [],
            "triton": {
                "kernel_name": "aten._scaled_dot_product_efficient_attention",
                "kernel_path": ""
            }
        }
    ],
    "gpu_info": {
        "name": "Tesla V100-PCIE-32GB",
        "compute_capability": [
            7,
            0
        ],
        "total_memory": 32510.6875,
        "clock_rate": 1380.0,
        "memory_clock_rate": 877.0,
        "num_multiprocessors": 80,
        "max_threads_per_block": 1024,
        "max_threads_per_multiprocessor": 2048,
        "warp_size": 32,
        "l2_cache_size": 6291456,
        "max_shared_memory_per_multiprocessor": 98304,
        "global_memory_bus_width": 4096
    }
}